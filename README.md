# GIMLeT â€“ Gestural Interaction Machine Learning Toolkit

A set of Max patches for gesture analysis, interactive machine learning, and gesture-sound interaction design. GIMLeT features a modular design that allows to easily share meaningfully structured data between several gesture tracking devices, machine learning, and sound synthesis modules.

## Installation
Download the .zip file, open it, copy the GIMLeT folder in your Max "Packages" folder. 
### Launching Example Patches
Launch Max, click on Extras->"GIMLeT examples" on the menu bar, choose an example. 
 
## Dependencies
NOTE: the required objects from these libraries are included in the package in order to make distribution easier.

- o.dot : OSC-centred multipardigm dynamic programming in Max  
  https://github.com/CNMAT/CNMAT-odot  
  
- modosc : real-time motion feature extraction  
  https://github.com/motiondescriptors/modosc  
  
- rapidmax : Max external for interactive machine learning  
  https://github.com/samparkewolfe/RapidMax (Mac)  
  https://github.com/MartinTownley/RapidMax_Windows   
  
- petra : Max package for granular synthesis  
  https://github.com/CircuitMusicLabs/petra  
  
- Gesture Variation Follower  
  https://github.com/bcaramiaux/ofxGVF  
  
- HfMT Optitrack OSC bridge  
  https://github.com/HfMT-ZM4/motion-tracking  

## Contact

mail[at]federicovisi[dot]com

www.federicovisi.com
